from pyspark.sql import SparkSession

# -------------------------------------
# 1. Configuration (Update with your actual paths and DB)
# -------------------------------------
raw_csv_path = "s3://your-raw-bucket/path/to/confidential_file.csv"
clean_parquet_path = "s3://your-consumption-bucket/path/cleaned_output/"
glue_database = "your_glue_db"
glue_table_name = "confidential_table"

# -------------------------------------
# 2. Spark session
# -------------------------------------
spark = SparkSession.builder.appName("CSVToGlueTable").getOrCreate()

# -------------------------------------
# 3. Read raw CSV with robust config
# -------------------------------------
df = spark.read.format("csv") \
    .option("header", "true") \
    .option("multiLine", "true") \
    .option("quote", "\"") \
    .option("escape", "\"") \
    .option("delimiter", ",") \
    .option("ignoreLeadingWhiteSpace", "true") \
    .option("ignoreTrailingWhiteSpace", "true") \
    .load(raw_csv_path)

print("✅ Raw CSV Schema:")
df.printSchema()

# ✅ Visual check of raw data
print("✅ Previewing raw CSV:")
display(df.limit(5))

# -------------------------------------
# 4. Reorder columns (MASKED VERSION)
# -------------------------------------
ordered_columns = [
    "col_01",
    "col_02",
    "col_03",
    "col_04",
    "col_05",
    "col_06",
    "col_07",
    "col_08",
    "col_09",
    "col_10",
    "col_11",
    "col_12",
    "col_13",
    "col_14",
    "col_15",
    "col_16",
    "col_17",
    "col_18"
]

df_cleaned = df.select([col for col in ordered_columns])

# ✅ Visual check of cleaned/reordered data
print("✅ Previewing cleaned and ordered data:")
display(df_cleaned.limit(5))

# -------------------------------------
# 5. Write to Parquet in clean S3 location
# -------------------------------------
df_cleaned.write.mode("overwrite").parquet(clean_parquet_path)
print(f"✅ Parquet written to: {clean_parquet_path}")

# -------------------------------------
# 6. Create Glue-compatible external table
# -------------------------------------
spark.sql(f"CREATE DATABASE IF NOT EXISTS {glue_database}")
spark.sql(f"DROP TABLE IF EXISTS {glue_database}.{glue_table_name}")

spark.sql(f"""
CREATE TABLE {glue_database}.{glue_table_name}
USING PARQUET
LOCATION '{clean_parquet_path}'
""")
print(f"✅ Glue table created: {glue_database}.{glue_table_name}")

# -------------------------------------
# 7. Read back from Glue table and verify
# -------------------------------------
df_glue = spark.sql(f"SELECT * FROM {glue_database}.{glue_table_name}")

# ✅ Final check of Glue table content
print("✅ Previewing data from Glue table:")
display(df_glue.limit(5))

# ✅ Optional: Verify round-trip data match
print("✅ Row count match:", df_cleaned.count() == df_glue.count())
df_cleaned.exceptAll(df_glue).show(truncate=False)