from pyspark.sql import SparkSession

# ----------------------------------------------------
# Configuration (update these paths accordingly)
# ----------------------------------------------------
raw_csv_path = "s3://your-raw-bucket/path/to/lu_servicingoverview.csv"
clean_parquet_path = "s3://your-consumption-bucket/lu_servicingoverview_cleaned/"
glue_database = "your_glue_db"
glue_table_name = "lu_servicingoverview"

# ----------------------------------------------------
# Spark session (Databricks auto-configures this)
# ----------------------------------------------------
spark = SparkSession.builder.appName("CSVToGlueTable").getOrCreate()

# ----------------------------------------------------
# Step 1: Read raw CSV from S3 with proper handling
# ----------------------------------------------------
df = spark.read.format("csv") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .option("quote", "\"") \
    .option("escape", "\"") \
    .option("multiLine", "true") \
    .option("delimiter", ",") \
    .option("ignoreLeadingWhiteSpace", "true") \
    .option("ignoreTrailingWhiteSpace", "true") \
    .load(raw_csv_path)

# ----------------------------------------------------
# Step 2: Display the data for confirmation
# ----------------------------------------------------
print("✅ Previewing parsed data from CSV:")
df.show(truncate=False)

# ----------------------------------------------------
# Step 3: Write to Parquet in clean S3 location
# ----------------------------------------------------
df.write.mode("overwrite").parquet(clean_parquet_path)
print(f"✅ Parquet written to {clean_parquet_path}")

# ----------------------------------------------------
# Step 4: Create Glue-compatible external table
# ----------------------------------------------------
spark.sql(f"CREATE DATABASE IF NOT EXISTS {glue_database}")

spark.sql(f"""
CREATE TABLE IF NOT EXISTS {glue_database}.{glue_table_name}
USING PARQUET
LOCATION '{clean_parquet_path}'
""")

print(f"✅ Glue table {glue_database}.{glue_table_name} created successfully.")

# ----------------------------------------------------
# Step 5: Validate final Glue table content
# ----------------------------------------------------
print("✅ Validating data from Glue table:")
df2 = spark.sql(f"SELECT * FROM {glue_database}.{glue_table_name}")
df2.show(truncate=False)