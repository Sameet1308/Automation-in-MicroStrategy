from pyspark.sql import SparkSession
import re

# -------------------------------------
# 1. Configuration (update with your real paths and DB/table names)
# -------------------------------------
raw_csv_path = "s3://your-raw-bucket/path/to/confidential_file.csv"
clean_parquet_path = "s3://your-consumption-bucket/path/cleaned_output/"
glue_database = "your_glue_db"
glue_table_name = "confidential_table"

# -------------------------------------
# 2. Start Spark session
# -------------------------------------
spark = SparkSession.builder.appName("CSVToGlueTableCleaned").getOrCreate()

# -------------------------------------
# 3. Read raw CSV with correct options
# -------------------------------------
df = spark.read.format("csv") \
    .option("header", "true") \
    .option("multiLine", "true") \
    .option("quote", "\"") \
    .option("escape", "\"") \
    .option("delimiter", ",") \
    .option("ignoreLeadingWhiteSpace", "true") \
    .option("ignoreTrailingWhiteSpace", "true") \
    .load(raw_csv_path)

print("✅ Raw CSV Schema:")
df.printSchema()

print("✅ Previewing raw CSV:")
display(df.limit(5))

# -------------------------------------
# 4. Clean column names (remove ()[]{} newline, spaces, special chars)
# -------------------------------------
def clean_column_name(col_name):
    return re.sub(r'[^a-zA-Z0-9_]', '_', col_name.strip()).lower()

cleaned_columns = [clean_column_name(col) for col in df.columns]
df_cleaned = df.toDF(*cleaned_columns)

print("✅ Cleaned column names:")
print(df_cleaned.columns)

# -------------------------------------
# 5. Reorder columns alphabetically (or keep custom order if known)
# -------------------------------------
ordered_columns = sorted(df_cleaned.columns)
df_cleaned = df_cleaned.select(ordered_columns)

print("✅ Previewing cleaned and ordered data:")
display(df_cleaned.limit(5))

# -------------------------------------
# 6. Write to Parquet in clean S3 path
# -------------------------------------
df_cleaned.write.mode("overwrite").parquet(clean_parquet_path)
print(f"✅ Cleaned Parquet written to: {clean_parquet_path}")

# -------------------------------------
# 7. Create Glue-compatible external table
# -------------------------------------
spark.sql(f"CREATE DATABASE IF NOT EXISTS {glue_database}")
spark.sql(f"DROP TABLE IF EXISTS {glue_database}.{glue_table_name}")

spark.sql(f"""
CREATE TABLE {glue_database}.{glue_table_name}
USING PARQUET
LOCATION '{clean_parquet_path}'
""")
print(f"✅ Glue table created: {glue_database}.{glue_table_name}")

# -------------------------------------
# 8. Read and display final Glue table output
# -------------------------------------
df_glue = spark.sql(f"SELECT * FROM {glue_database}.{glue_table_name}")
print("✅ Final Glue Table Output:")
display(df_glue.limit(5))

# -------------------------------------
# 9. Optional: Integrity check
# -------------------------------------
print("✅ Row count match:", df_cleaned.count() == df_glue.count())
df_cleaned.exceptAll(df_glue).show(truncate=False)