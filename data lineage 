# ======================================================================================
#  Databricks | Raw Parquet (S3) ➜ Delta (S3 consumption) ➜ Unity Catalog Table
#  Cleaned-up version: NO catalog/schema creation (those are admin-controlled)
# ======================================================================================

from pyspark.sql import SparkSession, functions as F
import traceback

spark = SparkSession.builder.getOrCreate()

# -----------------------------
# PARAMETERS (EDIT THESE)
# -----------------------------
CATALOG       = "my_catalog"          # e.g. "main"
SCHEMA        = "my_schema"           # e.g. "consumption"
TABLE_NAME    = "sales_txn"

RAW_PARQUET_PATH  = "s3://my-bucket/raw/mytbl/"
CONS_DELTA_PATH   = "s3://my-bucket/cons/mytbl/"

WRITE_MODE       = "overwrite"        # or "append"
PARTITION_COLS   = ["event_date"]     # [] if no partitioning
OVERWRITE_SCHEMA = True

RUN_OPTIMIZE   = True
ZORDER_COLS    = ["event_date", "id"] # [] if no Z-ORDER
RUN_VACUUM     = False
VACUUM_RETAIN_HOURS = 168             # 7 days

PRINT_SAMPLE_ROWS  = 5
SHOW_EXTENDED_DESC = True

# -----------------------------
# MAIN LOGIC
# -----------------------------
try:
    full_table_name = f"{CATALOG}.{SCHEMA}.{TABLE_NAME}"
    print(f"[INFO] Target UC table: {full_table_name}")
    print(f"[INFO] RAW (Parquet):   {RAW_PARQUET_PATH}")
    print(f"[INFO] CONS (Delta):    {CONS_DELTA_PATH}")

    # 1) Read RAW Parquet
    df_raw = spark.read.format("parquet").load(RAW_PARQUET_PATH)
    print("[INFO] RAW schema:")
    df_raw.printSchema()
    if PRINT_SAMPLE_ROWS > 0:
        df_raw.show(PRINT_SAMPLE_ROWS, truncate=False)

    # 2) (Optional) Transform
    df_out = df_raw.withColumn("ingest_ts", F.current_timestamp())
    if "event_ts" in df_out.columns and "event_date" not in df_out.columns:
        df_out = df_out.withColumn("event_date", F.to_date("event_ts"))

    # 3) Write to consumption prefix as Delta
    writer = df_out.write.format("delta").mode(WRITE_MODE)
    if OVERWRITE_SCHEMA:
        writer = writer.option("overwriteSchema", "true")
    if PARTITION_COLS:
        writer = writer.partitionBy(*PARTITION_COLS)
    writer.save(CONS_DELTA_PATH)
    print(f"[INFO] Delta written at {CONS_DELTA_PATH}")

    # 4) Register UC table (no schema creation, just table on location)
    spark.sql(f"""
        CREATE TABLE IF NOT EXISTS {full_table_name}
        USING DELTA
        LOCATION '{CONS_DELTA_PATH}'
    """)
    print(f"[INFO] UC table registered: {full_table_name}")

    # 5) Optional OPTIMIZE / Z-ORDER
    if RUN_OPTIMIZE:
        if ZORDER_COLS:
            spark.sql(f"OPTIMIZE {full_table_name} ZORDER BY ({', '.join(ZORDER_COLS)})")
        else:
            spark.sql(f"OPTIMIZE {full_table_name}")
        print("[INFO] OPTIMIZE completed.")

    # 6) Optional VACUUM
    if RUN_VACUUM:
        spark.sql(f"VACUUM {full_table_name} RETAIN {VACUUM_RETAIN_HOURS} HOURS")
        print("[INFO] VACUUM completed.")

    # 7) Sanity queries
    spark.sql(f"SELECT COUNT(*) FROM {full_table_name}").show()
    spark.sql(f"SELECT * FROM {full_table_name} LIMIT 20").show(truncate=False)

    # 8) Optional describe
    if SHOW_EXTENDED_DESC:
        spark.sql(f"DESCRIBE EXTENDED {full_table_name}").show(truncate=False)

    print("=== Pipeline completed successfully. ===")

except Exception as e:
    print("[ERROR] Pipeline failed:", e)
    traceback.print_exc()
    raise