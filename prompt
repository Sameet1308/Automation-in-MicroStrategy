# ============================================================================
# CELL 1: Install Packages
# ============================================================================
# %pip install requests>=2.32.0 pandas>=2.2.0 urllib3>=2.2.0


# ============================================================================
# CELL 2: MICROSTRATEGY EXTRACTOR - COMPLETE VERSION WITH VISUALIZATION-LEVEL OBJECTS
# ============================================================================

import requests
import urllib3
import pandas as pd
import warnings
from typing import Optional, Dict, List
from datetime import datetime
import os
import json

# Disable warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
warnings.filterwarnings('ignore')

# ============================================================================
# CONFIGURATION - CHANGE THESE VALUES
# ============================================================================

# MicroStrategy Connection (REQUIRED)
LIBRARY_URL = "https://your-server/MicroStrategyLibrary"  # CHANGE THIS
USERNAME = "your-username"                                  # CHANGE THIS
PASSWORD = "your-password"                                  # CHANGE THIS
PROJECT_ID = "your-project-id"                             # CHANGE THIS - REQUIRED

# Dossier Selection (OPTIONAL)
DOSSIER_ID = None  # Leave as None to extract ALL dossiers, or provide specific dossier ID

# CSV Output Settings
CSV_OUTPUT_PATH = "/dbfs/FileStore/mstr_extraction"       # Output path
CSV_FILENAME = "dossier_derived_objects_complete"         # Output filename

# ============================================================================
# EXTRACTOR CLASS
# ============================================================================

class MicroStrategyExtractor:
    
    def __init__(self, library_url: str, username: str, password: str, project_id: str):
        self.library_url = library_url.rstrip('/')
        self.username = username
        self.password = password
        self.project_id = project_id
        self.session = requests.Session()
        self.session.verify = False
        self.auth_token = None
        
    def authenticate(self) -> bool:
        """Authenticate with MicroStrategy"""
        try:
            auth_url = f"{self.library_url}/api/auth/login"
            response = self.session.post(
                auth_url,
                json={"username": self.username, "password": self.password},
                headers={"Content-Type": "application/json"}
            )
            response.raise_for_status()
            self.auth_token = response.headers.get('X-MSTR-AuthToken')
            return self.auth_token is not None
        except Exception as e:
            print(f"‚úó Authentication failed: {e}")
            return False
    
    def get_all_dossiers(self) -> List[Dict]:
        """Get all dossiers in the project"""
        try:
            response = self.session.get(
                f"{self.library_url}/api/searches/results",
                headers={
                    "X-MSTR-AuthToken": self.auth_token,
                    "X-MSTR-ProjectId": self.project_id
                },
                params={"type": 55, "limit": -1, "getTree": False, "isCrossCluster": False, "offset": 0}
            )
            response.raise_for_status()
            return response.json().get('result', [])
        except Exception as e:
            print(f"‚úó Error fetching dossiers: {e}")
            return []
    
    def get_dossier_definition(self, dossier_id: str) -> Optional[Dict]:
        """Get complete dossier definition"""
        try:
            response = self.session.get(
                f"{self.library_url}/api/v2/dossiers/{dossier_id}/definition",
                headers={
                    "X-MSTR-AuthToken": self.auth_token,
                    "X-MSTR-ProjectId": self.project_id
                }
            )
            response.raise_for_status()
            return response.json()
        except Exception as e:
            print(f"‚úó Error for dossier {dossier_id}: {e}")
            return None
    
    def extract_single_dossier(self, dossier_id: str, dossier_name: str = None) -> List[Dict]:
        """Extract all derived objects from a single dossier"""
        
        if not dossier_name:
            dossier_name = dossier_id
        
        print(f"  Processing: {dossier_name}")
        
        definition = self.get_dossier_definition(dossier_id)
        
        if not definition:
            print(f"  ‚úó Failed to get definition")
            return []
        
        # Extract from both datasets AND visualizations
        records = []
        records.extend(self._extract_from_datasets(dossier_id, definition.get('name', dossier_name), definition))
        records.extend(self._extract_from_visualizations(dossier_id, definition.get('name', dossier_name), definition))
        
        print(f"  ‚úì Extracted {len(records)} total objects")
        
        return records
    
    def extract_all_dossiers(self) -> List[Dict]:
        """Extract all dossiers in project"""
        
        dossiers = self.get_all_dossiers()
        print(f"Found {len(dossiers)} dossiers in project\n")
        
        all_records = []
        
        for idx, dossier in enumerate(dossiers, 1):
            dossier_id = dossier.get('id')
            dossier_name = dossier.get('name', 'Unknown')
            
            print(f"[{idx}/{len(dossiers)}] {dossier_name}")
            
            records = self.extract_single_dossier(dossier_id, dossier_name)
            all_records.extend(records)
        
        return all_records
    
    def _extract_from_datasets(self, dossier_id: str, dossier_name: str, definition: Dict) -> List[Dict]:
        """Extract objects from datasets (cube-level objects)"""
        
        records = []
        datasets = definition.get('datasets', [])
        
        for dataset in datasets:
            dataset_id = dataset.get('id', '')
            dataset_name = dataset.get('name', '')
            dataset_type = dataset.get('type', '')
            
            objects = dataset.get('availableObjects', [])
            
            for obj in objects:
                obj_type = obj.get('type', '')
                
                # Only metrics and attributes
                if obj_type not in ['metric', 'attribute']:
                    continue
                
                record = {
                    'dossier_id': dossier_id,
                    'dossier_name': dossier_name,
                    'source_type': 'dataset',  # NEW: identifies source
                    'source_location': f"Dataset: {dataset_name}",
                    'dataset_id': dataset_id,
                    'dataset_name': dataset_name,
                    'dataset_type': dataset_type,
                    'chapter_name': None,
                    'visualization_name': None,
                    'object_id': obj.get('id', ''),
                    'object_name': obj.get('name', ''),
                    'object_type': obj_type,
                    'object_subtype': obj.get('subType', ''),
                    'object_description': obj.get('description', ''),
                    'forms_count': None,
                    'form_names': None,
                    'extraction_timestamp': datetime.now().isoformat()
                }
                
                # Add forms for attributes
                if obj_type == 'attribute':
                    forms = obj.get('forms', [])
                    if forms:
                        record['forms_count'] = len(forms)
                        record['form_names'] = ', '.join([f.get('name', '') for f in forms])
                
                records.append(record)
        
        return records
    
    def _extract_from_visualizations(self, dossier_id: str, dossier_name: str, definition: Dict) -> List[Dict]:
        """Extract dashboard-level derived objects from visualizations"""
        
        records = []
        chapters = definition.get('chapters', [])
        
        for chapter in chapters:
            chapter_key = chapter.get('key', '')
            chapter_name = chapter.get('name', 'Unknown Chapter')
            
            pages = chapter.get('pages', [])
            
            for page in pages:
                visualizations = page.get('visualizations', [])
                
                for viz in visualizations:
                    viz_key = viz.get('key', '')
                    viz_name = viz.get('name', 'Unknown Visualization')
                    
                    # Check for derived metrics and attributes in visualization
                    derived_metrics = viz.get('metrics', [])
                    derived_attributes = viz.get('attributes', [])
                    
                    # Process derived metrics
                    for metric in derived_metrics:
                        metric_id = metric.get('id', '')
                        metric_name = metric.get('name', '')
                        
                        # Skip if this metric is from a dataset (not dashboard-derived)
                        if self._is_from_dataset(metric_id, definition):
                            continue
                        
                        record = {
                            'dossier_id': dossier_id,
                            'dossier_name': dossier_name,
                            'source_type': 'visualization',  # Dashboard-level
                            'source_location': f"Chapter: {chapter_name} > Viz: {viz_name}",
                            'dataset_id': None,
                            'dataset_name': None,
                            'dataset_type': None,
                            'chapter_name': chapter_name,
                            'visualization_name': viz_name,
                            'object_id': metric_id,
                            'object_name': metric_name,
                            'object_type': 'metric',
                            'object_subtype': 'dashboard_derived',
                            'object_description': metric.get('description', ''),
                            'forms_count': None,
                            'form_names': None,
                            'extraction_timestamp': datetime.now().isoformat()
                        }
                        
                        records.append(record)
                    
                    # Process derived attributes
                    for attr in derived_attributes:
                        attr_id = attr.get('id', '')
                        attr_name = attr.get('name', '')
                        
                        # Skip if this attribute is from a dataset
                        if self._is_from_dataset(attr_id, definition):
                            continue
                        
                        record = {
                            'dossier_id': dossier_id,
                            'dossier_name': dossier_name,
                            'source_type': 'visualization',  # Dashboard-level
                            'source_location': f"Chapter: {chapter_name} > Viz: {viz_name}",
                            'dataset_id': None,
                            'dataset_name': None,
                            'dataset_type': None,
                            'chapter_name': chapter_name,
                            'visualization_name': viz_name,
                            'object_id': attr_id,
                            'object_name': attr_name,
                            'object_type': 'attribute',
                            'object_subtype': 'dashboard_derived',
                            'object_description': attr.get('description', ''),
                            'forms_count': None,
                            'form_names': None,
                            'extraction_timestamp': datetime.now().isoformat()
                        }
                        
                        forms = attr.get('forms', [])
                        if forms:
                            record['forms_count'] = len(forms)
                            record['form_names'] = ', '.join([f.get('name', '') for f in forms])
                        
                        records.append(record)
        
        return records
    
    def _is_from_dataset(self, object_id: str, definition: Dict) -> bool:
        """Check if an object ID exists in datasets (to avoid duplicates)"""
        datasets = definition.get('datasets', [])
        
        for dataset in datasets:
            objects = dataset.get('availableObjects', [])
            for obj in objects:
                if obj.get('id') == object_id:
                    return True
        
        return False
    
    def logout(self):
        """Logout and cleanup"""
        try:
            if self.auth_token:
                self.session.post(
                    f"{self.library_url}/api/auth/logout",
                    headers={"X-MSTR-AuthToken": self.auth_token},
                    timeout=5
                )
        except:
            pass


# ============================================================================
# MAIN EXECUTION
# ============================================================================

print("="*80)
print("MICROSTRATEGY DOSSIER EXTRACTOR - COMPLETE (DATASET + DASHBOARD OBJECTS)")
print("="*80)

# Validate required parameters
if not LIBRARY_URL or not USERNAME or not PASSWORD or not PROJECT_ID:
    raise ValueError("‚ùå LIBRARY_URL, USERNAME, PASSWORD, and PROJECT_ID are required!")

print(f"\nüìã Configuration:")
print(f"  Library URL: {LIBRARY_URL}")
print(f"  Username: {USERNAME}")
print(f"  Project ID: {PROJECT_ID}")
if DOSSIER_ID:
    print(f"  Dossier ID: {DOSSIER_ID} (Single dossier mode)")
else:
    print(f"  Dossier ID: ALL (Extract all dossiers)")
print(f"  Output Path: {CSV_OUTPUT_PATH}")
print("="*80)

try:
    # Step 1: Initialize
    print("\nüöÄ Step 1: Initializing extractor...")
    extractor = MicroStrategyExtractor(LIBRARY_URL, USERNAME, PASSWORD, PROJECT_ID)
    
    # Step 2: Authenticate
    print("\nüîê Step 2: Authenticating...")
    if not extractor.authenticate():
        raise Exception("Authentication failed")
    print("‚úì Authenticated successfully")
    
    # Step 3: Extract dossiers
    print("\nüìä Step 3: Extracting dossier definitions...")
    print("-" * 80)
    
    if DOSSIER_ID:
        # Single dossier mode
        print(f"Extracting single dossier: {DOSSIER_ID}\n")
        records = extractor.extract_single_dossier(DOSSIER_ID)
    else:
        # All dossiers mode
        print("Extracting ALL dossiers in project\n")
        records = extractor.extract_all_dossiers()
    
    # Step 4: Logout
    extractor.logout()
    
    if not records:
        raise Exception("No data extracted - check if dossier has derived objects")
    
    # Step 5: Create DataFrame
    print("\n" + "="*80)
    print("‚úÖ EXTRACTION COMPLETE")
    print("="*80)
    
    df = pd.DataFrame(records)
    
    # Define column order
    columns = [
        'dossier_id',
        'dossier_name',
        'source_type',
        'source_location',
        'dataset_id',
        'dataset_name',
        'dataset_type',
        'chapter_name',
        'visualization_name',
        'object_id',
        'object_name',
        'object_type',
        'object_subtype',
        'object_description',
        'forms_count',
        'form_names',
        'extraction_timestamp'
    ]
    
    df = df[columns]
    
    # Statistics
    print(f"\nüìà Statistics:")
    print(f"  Total Records: {len(df)}")
    print(f"  Unique Dossiers: {df['dossier_name'].nunique()}")
    print(f"  Dataset-Level Objects: {len(df[df['source_type'] == 'dataset'])}")
    print(f"  Dashboard-Level Objects: {len(df[df['source_type'] == 'visualization'])}")
    print(f"  Total Metrics: {len(df[df['object_type'] == 'metric'])}")
    print(f"  Total Attributes: {len(df[df['object_type'] == 'attribute'])}")
    
    # Step 6: Export to CSV
    print("\n" + "="*80)
    print("üìÅ EXPORTING TO CSV")
    print("="*80)
    
    os.makedirs(CSV_OUTPUT_PATH, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_file = f"{CSV_OUTPUT_PATH}/{CSV_FILENAME}_{timestamp}.csv"
    
    df.to_csv(csv_file, index=False, encoding='utf-8')
    
    print(f"\n‚úì CSV File Created: {os.path.basename(csv_file)}")
    print(f"‚úì Location: {CSV_OUTPUT_PATH}")
    print(f"‚úì Total Rows: {len(df)}")
    
    # Step 7: Show sample
    print("\n" + "="*80)
    print("üìä SAMPLE DATA (First 10 rows)")
    print("="*80)
    
    sample_cols = ['dossier_name', 'source_type', 'object_name', 'object_type', 'source_location']
    print("\n" + df[sample_cols].head(10).to_string(index=False))
    
    # Step 8: Summary by source type
    print("\n" + "="*80)
    print("üìã SUMMARY BY SOURCE TYPE")
    print("="*80)
    
    summary = df.groupby(['dossier_name', 'source_type']).size().reset_index(name='count')
    print("\n" + summary.to_string(index=False))
    
    # Final message
    print("\n" + "="*80)
    print("‚úÖ SUCCESS!")
    print("="*80)
    print(f"\nüìÑ CSV File: {os.path.basename(csv_file)}")
    print(f"üìÇ Location: {CSV_OUTPUT_PATH}")
    print(f"\nüí° To download:")
    print(f"   1. Go to Databricks UI")
    print(f"   2. Click: Data ‚Üí FileStore")
    print(f"   3. Navigate to: {CSV_OUTPUT_PATH.replace('/dbfs/FileStore/', '')}")
    print(f"   4. Click on the file to download")
    print("\n‚ú® Now includes BOTH dataset-level AND dashboard-level derived objects!")

except Exception as e:
    print("\n" + "="*80)
    print("‚ùå ERROR OCCURRED")
    print("="*80)
    print(f"\nError: {str(e)}")
    import traceback
    traceback.print_exc()
    raise

print("\n" + "="*80)
